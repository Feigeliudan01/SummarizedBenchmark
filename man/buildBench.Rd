% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/buildBench.R
\name{buildBench}
\alias{buildBench}
\title{Make SummarizedBenchmark from BenchDesign}
\usage{
buildBench(bd, data = NULL, truthCols = NULL, ftCols = NULL,
  sortIDs = FALSE, catchErrors = TRUE, parallel = FALSE,
  BPPARAM = bpparam())
}
\arguments{
\item{bd}{\code{BenchDesign} object.}

\item{data}{Data set to be used for benchmarking, will take priority over
data set originally specified to BenchDesign object. 
Ignored if NULL. (default = NULL)}

\item{truthCols}{Character vector of column names in data set corresponding to
ground truth values for each assay. If specified, column will be added to
the \code{groundTruth} DataFrame for the returned SummarizedBenchmark object.
If the \code{BenchDesign} includes only a single assay, the same name
will be used for the assay. If the \code{BenchDesign} includes multiple assays,
to map data set columns with assays, the vector must have names corresponding
to the assay names specified to the \code{post} parameter at each
\code{addMethod} call. (default = NULL)}

\item{ftCols}{Vector of character names of columns in data set that should be
included as feature data (row data) in the returned SummarizedBenchmark
object. (default = NULL)}

\item{sortIDs}{Whether the output of each method should be merged using IDs.
If TRUE, each method must return a named vector or list. The names will be
used to align the output of each method in the returned SummarizedBenchmark.
Missing values will be set to NA. This can be useful if the different methods
return overlapping, but not identical, results. If \code{truthCols} is also
specified, and sorting by IDs is necessary, rather than specifying 'TRUE',
specify the string name of a column in the \code{data} to use to sort the
method output to match the order of \code{truthCols}. (default = FALSE)}

\item{catchErrors}{logical whether errors produced by methods during evaluation
should be caught and printed as a message without stopping the entire
build process. (default = TRUE)}

\item{parallel}{Whether to use parallelization for evaluating each method.
Parallel execution is performed using \code{BiocParallel}. Parameters for
parallelization should be specified with \code{BiocParallel::register} or
through the \code{BPPARAM} parameter. (default = FALSE)}

\item{BPPARAM}{Optional \code{BiocParallelParam} instance to be used when
\code{parallel} is TRUE. If not specified, the default instance from the
parameter registry is used.}
}
\value{
\code{SummarizedBenchmark} object with single assay
}
\description{
Function to evaluate \code{BenchDesign} methods on supplied
data set to generate a \code{SummarizedBenchmark}. In addition
to the results of applying each method on the data, the returned
\code{SummarizedBenchmark} also includes metadata for the methods
in the \code{colData} of the returned object, metadata for the
data in the \code{rowData}, and the session information generated
by \code{sessionInfo()} in the \code{metadata}.
}
\details{
Parallelization is performed across methods. Therefore, there is no benefit to
specifying more cores than the total number of methods in the \code{BenchDesign}
object.
By default, errors thrown by individual methods in the \code{BenchDesign} are caught
during evaluation and handled in a way that allows \code{buildBench} to continue
running with the other methods. The error is printed as a message, and the corresponding
column in the returned \code{SummarizedBenchmark} object is set to NA. Since
many benchmarking experiments can be time and computationally intensive, having to rerun
the entire analysis due to a single failed method can be frustrating. Default error catching
was included to alleviate these frustrations. However, if this behavior is not desired,
setting \code{catchErrors = FALSE} will turn off error handling.
}
\examples{
## with toy data.frame
df <- data.frame(pval = rnorm(100))
bench <- BenchDesign(df)

## add methods
bench <- addMethod(bench, label = "bonf", func = p.adjust,
                   params = rlang::quos(p = pval, method = "bonferroni"))
bench <- addMethod(bench, label = "BH", func = p.adjust,
                   params = rlang::quos(p = pval, method = "BH"))

## evaluate benchmark experiment
sb <- buildBench(bench)

## evaluate benchmark experiment w/ data sepecified
sb <- buildBench(bench, data = df)

}
\author{
Patrick Kimes
}

---
title: "SummarizedBenchmark: Bioconductor infrastructure for method benchmarking"
author: "Alejandro Reyes"
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SummarizedBenchmark')`"
abstract: >
  SummarizedExperiment package version: `r packageVersion("SummarizedBenchmark")`
output: 
  BiocStyle::html_document
bibliography: library.bib
vignette: >
  %\VignetteIndexEntry{Comparing methods using the SummarizedBenchmarks package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Constructing a __SummarizedBenchmark__ object

This vignette uses data from the `r BiocStyle::Biocpkg("iCOBRA")` package [@Soneson_2016] to 
demonstrate the __SummarizedBenchmark__ package. This dataset contains differential 
expression results of three different methods (`r BiocStyle::Biocpkg("limma")`, `r BiocStyle::Biocpkg("edgeR")` 
and `r BiocStyle::Biocpkg("DESeq2")`) applied to a simulated RNA-seq dataset.

```{r cobraData, message=FALSE, warning=FALSE}
library(SummarizedBenchmark)
library(iCOBRA)
data(cobradata_example)
```

The process of constructing a __SummarizedBenchmark__ object is similar to the one 
used to construct a __SummarizedExperiment__ object. In the __SummarizedBenchmark__ object, 
each output of the methods is considered a different *assay*. For example, using the differential
expression dataset example, we can define two assays, q-values and estimated log fold changes.
For each *assay*, we arrange the output of the different methods as a matrix where each
column corresponds to a method and each row corresponds to each feature (in this case, genes).
We will need a list in which each of it's element corresponds to an assay.


```{r arrangeLists}
assays <- list(
  qvalue=cobradata_example@padj,
  logFC=cobradata_example@score )
assays[["qvalue"]]$DESeq2 <- p.adjust(cobradata_example@pval$DESeq2, method="BH")
head( assays[["qvalue"]], 3)
head( assays[["logFC"]], 3)
```

Since these are simulated data, we know the ground truths for both assays.
We can format these as a _DataFrame_ where each column corresponds to an assay
and each row corresponds to a feature.

```{r groundTruths}
library(S4Vectors)
groundTruth <- DataFrame( cobradata_example@truth[,c("status", "logFC")] )
colnames(groundTruth) <- names( assays )
groundTruth <- groundTruth[rownames(assays[[1]]),]

head( groundTruth )
```

In cases where ground truths are not available, these values will be set to *NA* in 
the __SummarizedBenchmark__ object.

We will also need to specify the annotation of the 
assays. To do this, we need to prepare a __DataFrame__ that contains one row
for each column of the assays. For our example, we create a _DataFrame_ where
the column *method* contains the names of the different methods. 

```{r buildColData}
colData <- DataFrame( method=colnames(assays[[1]]) )
colData
```

For this example, we only have software names. However,
additional columns could contain, for example, the software versions or the parameters
used to run them.

Now we can construct a __SummarizedBenchmark__ object.

```{r buildSB}
sb <- SummarizedBenchmark(
  assays=assays, colData=colData,
  groundTruth=groundTruth )
sb
```

Notice that the __SummarizedBenchmark__ object is similar to a __SummarizedExperiment__ 
object. The main differences are that the __SummarizedBenchmark__ object contains ground 
truth values in the _rowData_ and an additional slot containing functions that define 
performance metrics.

```{r}
head( rowData(sb) )
sb@performanceFunctions
```

# Defining performance metrics

Since different benchmarks might use different metrics to evaluate the 
performance of a method, the __SummarizedBenchmark__ class provides a 
flexible way to define performance metrics. We can define a new performance 
metric using the function *addPerformanceMetric* by providing a __SummarizedBenchmark__ 
object, a name of the metric, an *assay* corresponding to the performance 
metric, and the function that defines it. Importantly, the function must 
contain the following two arguments: query (refering to a vector of values 
being evaluated, i.e. the output of one method) and truth (refering to the vector of 
ground truths). If further arguments are provided to the performance 
function, these must contain default values. 

For our example, we define the performance metric *TPR* (True Positive Rate) 
that calculates the fraction of true positives recovered given an *alpha* value.

```{r addPerformanceFunction}
sb <- addPerformanceFunction(
  object=sb,
  assay="qvalue",
  evalMetric="TPR",
  evalFunction = function( query, truth, alpha=0.1 ){
    goodHits <- sum( (query < alpha) & truth == 1 )
    goodHits / sum(truth == 1)
    }
)
```

Similarly, the code below includes two additional metrics, the jaccard index, which uses 
the *qvalue* assay, and the euclidean distance between the estimated log fold changes and 
the true fold changes, which uses the *logFC* assay. 

```{r morePerformanceFunctions}
sb <- addPerformanceFunction(
  object=sb,
  evalMetric="jaccardIndex",
  assay="qvalue",
  evalFunction = function( query, truth, alpha=0.1){
    goodHits <- sum( xor( query < alpha, as.logical( truth ) ) )
    goodHits / length( truth )
  }
)


sb <- addPerformanceFunction(
  object=sb,
  evalMetric="EucledianDist",
  assay="logFC",
  FUN <- function( query, truth ){
    sqrt( sum( (query - truth)^2 ) )
  }
)
```

# Estimating the performance metrics

Once having defined all the performance metrics, there are
several ways to calculate these. 

An option is to call the function _estimatePerformanceMetrics_ 
to calculate these for each performance metrics defined 
__SummarizedBenchmark__ object. Several parameters for the
performance functions could be passed here. In this case,
we want the performance functions to be estimated with 
several _alpha_ values.

```{r}
resWide <- estimatePerformanceMetrics( sb, alpha=c(0.05, 0.1, 0.2) )
resWide
```

By default, this function will return a _DataFrame_, where the
parameters of the performance function are stored in its
_elementMetadata_

```{r elWide}
elementMetadata(resWide)
```

However, we could also specify the parameter _addColData_ for these
results to be stored in the _colData_ of the __SummarizedBenchmark__
object

```{r}
sb <- estimatePerformanceMetrics( sb, alpha=c(0.05, 0.1, 0.2), addColData=TRUE)
colData( sb )
```

If the user prefers tidier formats, by setting the parameter `tidy=TRUE` 
the function will return a long-formated version of the results. 

```{r}
estimatePerformanceMetrics( sb, tidy=TRUE )
```

As an alternative to get the same _data.frame_ the previous chunk, 
we can call the function _tidyUpMetrics_ on the saved results from
a __SummarizedBenchmark__ object

```{r}
tidyUpMetrics(sb)
```

# References

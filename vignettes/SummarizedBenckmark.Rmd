---
title: "SummarizedBenchmark: Bioconductor infrastructure for method benchmarking"
author: "Alejandro Reyes"
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SummarizedBenchmark')`"
abstract: >
  DESeq2 package version: `r packageVersion("SummarizedBenchmark")`
output: 
  BiocStyle::html_document2
#  bibliography: library.bib
vignette: >
  %\VignetteIndexEntry{Comparing methods using the SummarizedBenchmarks package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Constructing a __SummarizedBenchmark__ object

This vignette uses data from the `r BiocStyle::Biocpkg("iCOBRA")` package [see @Soneson_2016] to 
demonstrate the __SummarizedBenchmark__ package. This dataset contains differential 
expression results of three different methods (`r BiocStyle::Biocpkg("limma")`, `r BiocStyle::Biocpkg("edgeR")` 
and `r BiocStyle::Biocpkg("DESeq2")`) applied to a simulated RNA-seq dataset.

```{r cobraData}
library(SummarizedBenchmark)
library(iCOBRA)
data(cobradata_example)
```

The process of constructing a __SummarizedBenchmark__ object is almost the same as the one 
used to construct a __SummarizedExperiment__ object. In the __SummarizedBenchmark__ object, 
each output variable is considered a different *assay*. For example, using the differential
expression dataset example, we can define two assays, the qvalues and the estimated logFC.
For each *assay*, we arrange the output of the different methods as a matrix where each
column corresponds to a method and each row corresponds to each feature (in this case, genes).


```{r arrangeLists}
assays <- list(
  qvalue=cobradata_example@padj,
  logFC=cobradata_example@score )
assays[["qvalue"]]$DESeq2 <- p.adjust(cobradata_example@pval$DESeq2, method="BH")
head( assays[["qvalue"]], 3)
```

Since these are simulated data, we know the ground truths for both assays. If these is not 
available, the ground truths will be set to *NA* in the __SummarizedBenchmark__ object.

```{r groundTruths}
library(S4Vectors)
groundTruth <- DataFrame( cobradata_example@truth[,c("status", "logFC")] )
colnames(groundTruth) <- names( assays )
groundTruth <- groundTruth[rownames(assays[[1]]),]

head( groundTruth )
```

We will also need to specify the annotation of the columns of the 
assays. In this case, we prepare a __DataFrame__ that contains one row
for each column of the assays. For this example, we only add there the
software name, but here would be an ideal place to store software version
and parameters used to run each software.


```{r buildColData}

colData <- DataFrame( method=colnames(assays[[1]]) )
colData
```

Now have all the minimal things to construct a __SummarizedBenchmark__ object.

```{r buildSB}
sb <- SummarizedBenchmark(
  assays=assays, colData=colData,
  groundTruth=groundTruth )
sb
```

Notice that the __SummarizedBenchmark__ object looks very similar to a __SummarizedExperiment__ object.
However, it contains an columns in _rowData_ that specify the ground truths and an additional slot
containing _performanceMetrics_. 

## Defining performance metrics

Since different benchmarks might focus on different metrics to evaluate the performance of each methods, 
the __SummarizedBenchmark__ object provides a flexible way to define evaluation metrics. We can define
a performance metric by providing a __SummarizedBenchmark__ object, a name of the metric, an *assay* 
corresponding to the performance metric, and the function that defines it. Importantly, the function 
must contain the following arguments: query (refering to a vector of values being evaluated, i.e. the output of 
one method) and truth (refering to the vector of ground truths). If further arguments are provided to the
performance function, these must contain default values. 

Below, we define the performance metric *TPR* (True Positive Rate) calculates the fraction of true positives
recovered given an *alpha* value.

```{r addPerformanceFunction}
sb <- addPerformanceFunction(
  object=sb,
  assay="qvalue",
  evalMetric="TPR",
  evalFunction = function( query, truth, alpha=0.1 ){
    goodHits <- sum( (query < alpha) & truth == 1 )
    goodHits / sum(truth == 1)
    }
)
```

Similarly, the code below includes two additional metrics, the jaccard index using the
*qvalue* assay and the euclidean distance between the estimated log fold changes and 
the true fold changes (the *logFC* assay). 

```{r morePerformanceFunctions}
sb <- addPerformanceFunction(
  object=sb,
  evalMetric="jaccardIndex",
  assay="qvalue",
  evalFunction = function( query, truth, alpha=0.1){
    goodHits <- sum( xor( query < alpha, as.logical( truth ) ) )
    goodHits / length( truth )
  }
)


sb <- addPerformanceFunction(
  object=sb,
  evalMetric="EucledianDist",
  assay="logFC",
  FUN <- function( query, truth ){
    sqrt( sum( (query - truth)^2 ) )
  }
)
```


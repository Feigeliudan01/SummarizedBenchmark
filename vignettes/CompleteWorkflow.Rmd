---
title: "Benchmarking with SummarizedBenchmark"
author: "Patrick K. Kimes, Alejandro Reyes"
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SummarizedBenchmark')`"
abstract: >
  "When performing a data analysis in R, users are often presented with multiple packages and methods for accomplishing the same task. Benchmarking the performance of these different methods on real and simulated data sets is a common way of learning about the relative strengths and weaknesses of each approach. However, as the number of tools and parameters being compared increases, keeping track of output and how it was generated can quickly become messy. The `SummarizedBenchmark` package provides a framework for organizing benchmark comparisons. The This vignette introduces the general approach and features of the package using several examples of varying complexity. SummarizedBenchmark package version: `r packageVersion("SummarizedBenchmark")`"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
    fig_width: 5
vignette: >
  %\VignetteIndexEntry{Benchmarking with SummarizedBenchmark}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo=FALSE, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE,
                      cache = TRUE,
                      dev = "png",
                      message = FALSE,
                      error = FALSE,
                      warning = TRUE)
```

# Introduction

With `SummarizedBenchmark`, a complete benchmarking workflow is comprised of three primary components: **(1)** data, **(2)** methods, and **(3)** performance metrics. The first two (_data_ and _methods_) are necessary for carrying out the benchmark experiment, and the last (_performance metrics_) is essential for evaluating the results of the experiment. Following this approach, the `SummarizedBenchmark` package defines and makes use of two types of objects: **BenchDesign** objects and **SummarizedBenchmark** objects. BenchDesign objects contain (intuitively) the design of the benchmark experiment, namely the _data_ and _methods_. After constructing a BenchDesign, the experiment can be executed, and the results returned as a SummarizedBenchmark. SummarizedBenchmark objects extend the Bioconductor `SummarizedExperiment` class, but with the additional capability of working with _performance metrics_.  

The basic framework is illustrated in the figure below. Blue components must be specified by users. First, a BenchDesign is created with _data_ and _methods_. Next a SummarizedBenchmark is created with the method outputs, and paired with _performance metrics_, again specified by the uesr. For convenience, several default _performance metrics_ have been implemented in the package which can be accessed with several simple commands.  

![basic benchmarking class relationship](summarizedbenchmark-figure1.png)

In this vignette, we first illustrate the basic use of both the **BenchDesign** and **SummarizedBenchmark** classes with a simple comparison of methods for p-value correction in the context of multiple hypothesis testing. Then, we describe more advanced features of the package with a case study comparison of common methods for differential expression analysis.

## Related Work

Related frameworks for benchmarking methods have been proposed in R, notably: [`iCOBRA`](https://github.com/markrobinsonuzh/iCOBRA) (available on Bioconductor) and [`dscr`](https://github.com/stephens999/dscr) (available on GitHub). The SummarizedBenchmark approach differs from both `iCOBRA` and `dscr` in several important ways.

First, while the `iCOBRA` package provides support for evaluating the results of a benchmark experiment, it does not include infrastructure for experiment design and execution. In this way, while no equivalent to the `BenchDesign` class exists as part of the `iCOBRA` package, the `iCOBRA` "`COBRAData`" class is similar to our "`SummarizedBenchmark`" class. However, by extending the standard Bioconductor `SummarizedExperiment` class, the `SummarizedBenchmark` class has the advantage of following a well-known data strucutre and facilitating easier downstream analyses. Finally, it may not even be reasonable to directly compare `iCOBRA` and `SummarizedBenchmark`, as `iCOBRA` was developed explicitly for the purpose of comparing results of _"binary classification and ranking methods"_ with a web application (Shiny app) for interactive analyses.

The `SummarizedBenchmark` framework is much closer in design and theory to the `dscr` package. Similar to `Summarizedbenchmark`, the `dscr` framework requires three components: **(1)** data simulators, **(2)** methods, and **(3)** score functions. However, `SummarizedBenchmark` differs from `dscr` notably in implementation. In addition to extending the `SummarizedExperiment` class for the `SummarizedBenchmark` class, the storage and execution of methods in `BenchDesign` makes use of the "**tidy eval**" approach recently proposed and implemented in the `rlang` package. 

# Quickstart Case Study

```{r}
library("SummarizedBenchmark")
library("magrittr")
```

To illustrate the use of the `BenchDesign` class, we use the `tdat` data set included with this package. 

```{r}
data(tdat)
```

The data set is a data.frame containing the results of 50 two-sample t-tests. The tests were performed using independently simulated sets of 20 observations drawn from a single standard Normal distribution (when `H = 0`) or two mean-shifted Normal distributions (when `H = 1`).

```{r}
head(tdat)
```

Several approaches have been proposed and implemented to compute *adjusted p-values* with the goal of controling the total number of false discoveries across the collection of tests. In this example, we compare three such methods:

1. Bonferroni correction (`p.adjust` w/ `method = "bonferroni"`),
2. Benjamini-Hochberg (`p.adjust` w/ `method = "BH"`), and
3. Storey's FDR q-value (`qvalue::qvalue`).


## Standard (Non-SummarizedBenchmark) Approach

First, consider how benchmarking the three methods might look without the `BenchDesign` framework.  

To compare methods, each is applied to `tdat` and the results are stored in separate variables. 

```{r}
adjp_bonf <- p.adjust(p = tdat$pval, method = "bonferroni")

adjp_bh <- p.adjust(p = tdat$pval, method = "BH")

qv <- qvalue::qvalue(p = tdat$pval)
adjp_qv <- qv$qvalues
```

Since the output of each method is a vector of length 50 (the number of hypothesis tests), to keep things clean, they can be combined into a single data.frame.

```{r}
adjp <- cbind.data.frame(adjp_bonf, adjp_bh, adjp_qv)
head(adjp)
```

The data.frame of adjusted p-values can be used to evaluate each method. This can be done either by directly parsing the table, or by using a framework like `iCOBRA`. Additionally, the data.frame can be saved as a `RDS` or `Rdata` object for future reference, eliminating the need for recomputing on the original data.  

While this approach can work for smaller comparisons, it can quickly become overwhelming and unweildy as the number of methods and parameters included in the benchmark increases. Furthermore, once each method is applied and the final data.frame (`adj`) is constructed, there is no way to determine *how* each adjusted p-value was calculated. While an informative name can be used to "label" each method (as done above), this does not capture the full complexity, e.g. parameters and context, where the function was evaluated. One solution might involve manually recording function calls and parameters in a separate data.frame with the hope of maintaining synchrony with the output data.frame. However, this is prone to errors, e.g. during fast "copy and paste" operations or additions and delations of parameter combinations. An alternative (and hopefully better) solution, is to use the framework of the `SummarizedBenchmark` package.

## SummarizedBenchmark Approach

In the `SummarizedBenchmark` approach, a `BenchDesign` is first constructed with the data as the sole input. (A `BenchDesign` can also be constructed without any data input. This approach is described in a later section.)

```{r}
b <- BenchDesign(tdat)
```

Then, each method of interest is added to the `BenchDesign` using `addBMethod()`. 

```{r}
b <- BenchDesign(tdat)
b <- addBMethod(b, blabel = "bonf", bfunc = p.adjust,
                p = pval, method = "bonferroni")
```

At a minimum, `addBMethod()` requires specifying three parameters:

1. `b`: the `BenchDesign` object to modify,
2. `blabel`: a character name for the method, and
3. `bfunc`: the function to be called.

After the minimum parameters are specified, any parameters needed by the `bfunc` method should be passed as named variables, e.g. `p = pval, method = "bonferroni"`. Notice that `pval` **does not** need to be called as `tdat$pval`.  

The process of adding methods can be written more concisely using the pipe operators from the `magrittr` package.

```{r}
b <- b %>% addBMethod(blabel = "BH", bfunc = p.adjust,
                      p = pval, method = "BH")
```

For some methods, such as the q-value approach above, it may be necessary to call a "post-processing" function on the primary method to extract the desired output (here, the q-values). This should be specified using the optional `bpost` parameter. 

```{r}
b %<>% addBMethod("qv", qvalue::qvalue,
                  p = pval,
                  bpost = function(x) { x$qvalues })
```

Now, the `BenchDesign` object contains three methods. This can be verified using the `showBMethods()` function.

```{r}
showBMethods(b)
```

While the bench now includes all the information necessary for performing the benchmarking study, the actual adjusted p-values have not yet been calculated. To do this, we simply call `buildBench()`. This returns a `SummarizedBenchmark` object with a single "assay" containing the table of adjusted p-values.

```{r}
sb <- buildBench(b)
sb
```

```{r}
head(assay(sb))
```

The `colData` of the same object contains information on the methods (columns) of each assay, with each row of the `colData` corresponding to a column in the `assay`. The `colData` includes columns for the functions and parameters specified for each method with `addBMethod` (`bfunc, bpost, blabel, param.*`). Additionally, several other columns are added during the `buildBench` process, including a column specifying whether `bfunc` was an anonymous function (`bfunc_anon`), and columns for the package name and version of `bfunc` if available (`pkg_name`, `pkg_vers`).

```{r}
colData(sb)
```

The resulting object can now be saved to disk with the method information tied directly to the resulting output. Additionally, comparison of the adjusted p-values can now be carried out using downstream methods using the `SummarizedBenchmark` framework.

**Include SummarizedBenchmark usage here.**

# Differential Expression Case Study

```{r}
library("limma")
library("edgeR")
library("DESeq2")
library("tximport")
```

In this more advanced case study, we use a simulated data set from _Soneson et al. (2016)_ to demonstrate how the `SummarizedBenchmark` package can be used to benchmark methods for differential expression analysis, namely DESeq2, edgeR, and limma-voom. The simulated data set includes 6 samples of three replicates each from two conditions. For each sample, transcript-level expression is provided as transcriper per-million (TPM) for 15,677 transcripts and 3,858 genes from human chromosome 1 (Ensembl GRCh37.71). A more complete description of the data, including code for how the data ws generated, is available in the supplementary materials of _Soneson et al. (2016)_ [here](http://dx.doi.org/10.5256/f1000research.7563.d114722).

## Preparing the Data

The data has been made available for download from ArrayExpress. Expression data for each sample is provided in the RSEM output format. Additionally, corresponding ground truth information using for the simulation is also available, including transcript differential expression status.

First, we download and read in the transcript-level TPMs using the `tximport` package.

```{r download-data}
d <- tempdir()
download.file(url = paste0("https://www.ebi.ac.uk/arrayexpress/files/",
                           "E-MTAB-4119/E-MTAB-4119.processed.3.zip"),
              destfile = file.path(d, "samples.zip"))
unzip(file.path(d, "samples.zip"), exdir = d)

fl <- list.files(d, pattern = "*_rsem.txt", full.names=TRUE)
names(fl) <- gsub("sample(.*)_rsem.txt", "\\1", basename(fl))
txi <- tximport(fl, txIn = TRUE, txOut = TRUE,
                geneIdCol = "gene_id",
                txIdCol = "transcript_id",
                countsCol = "expected_count",
                lengthCol = "effective_length",
                abundanceCol = "TPM",
                countsFromAbundance = "scaledTPM")
```

Next, we obtain and load the ground truth information that can be used for evaluating the results of the differential expression analysis.

```{r}
## download transcript-level truth data
download.file(url = paste0("https://www.ebi.ac.uk/arrayexpress/files/",
                           "E-MTAB-4119/E-MTAB-4119.processed.2.zip"),
              destfile = file.path(d, "truth.zip"))
unzip(file.path(d, "truth.zip"), exdir = d)

## read in expected results for simulated data
truthdat <- readr::read_tsv(file.path(d, "truth_transcript.txt"))
```

## SummarizedBenchmark Approach

### Benchmark Set-Up and Execution

We begin the benchmarking process by creating our `BenchDesign` object with the data set. The `BenchDesign` can be initialized with a data.frame (as in the case study above), or more generally, with a list object. In this case study, since methods for differential expression require more than just the expression counts, e.g. the experimental design, we construct a list containing each of these objects as a named entry.

The scaled TPM values must be rounded before passing to the differential expression methods.

```{r}
mycounts <- round(txi$counts)
```

The experimental design, here simply the conditions for each sample, must also be created.

```{r}
mycoldat <- data.frame(condition = factor(rep(c(1, 2), each = 3)))
rownames(mycoldat) <- colnames(mycounts)
```

The data object for the benchmark experiment is now constructed with both data objects, along with some ground truth information ("status": the true presence or absense of differential expression between conditions, "lfc": the expected log-fold change between conditions).

```{r}
mydat <- list(coldat = mycoldat,
              cntdat = mycounts,
              status = truthdat$status,
              lfc = truthdat$logFC)
```

As before, the `BenchDesign` is constructed with the data as the sole input.

```{r}
bd <- BenchDesign(mydat)
```

For simplicity, here, we focus on comparing only the p-values returned by each method (DESeq2, edgeR, limma-voom) after testing for differential expression between the two conditions across the `r nrow(mycounts)` transcripts. However, later in this vignette, we also provide an example for comparing multiple metrics (p-values and log-fold change) as part of a single benchmark experiment.  

Since each method requires running multiple steps, we must write wrapper functions which return only the vector p-values for each method. 

```{r}
deseq2_pvals <- function(countData, colData, design, contrast) {
    dds <- DESeqDataSetFromMatrix(countData,
                                  colData = colData,
                                  design = design)
    dds <- DESeq(dds)
    res <- results(dds, contrast = contrast)
    res$pvalue
}

edgeR_pvals <- function(countData, group, design) {
    y <- DGEList(countData, group = group)
    y <- calcNormFactors(y)
    des <- model.matrix(design)
    y <- estimateDisp(y, des)
    fit <- glmFit(y, des)
    lrt <- glmLRT(fit, coef=2)
    lrt$table$PValue
}

voom_pvals <- function(countData, group, design) {
    y <- DGEList(countData, group = group)
    y <- calcNormFactors(y)
    des <- model.matrix(design)
    y <- voom(y, des)
    eb <- eBayes(lmFit(y, des))
    eb$p.value[, 2]
}
```

Next, each method is added to `bd` using `addBMethod` and the corresponding wrapper function passed as the `bfunc` parameter. (For a review of the basic usage of `addBMethod`, revisit the Quickstart Case Study in the previous section.) We again use the pipe notation for compactness.

```{r}
bd <- bd %>%
    addBMethod(blabel = "deseq2",
               bfunc = deseq2_pvals,
               countData = cntdat,
               colData = coldat, 
               design = ~condition,
               contrast = c("condition", "2", "1")) %>%
    addBMethod(blabel = "edgeR",
               bfunc = edgeR_pvals,
               countData = cntdat,
               group = coldat$condition,
               design = ~coldat$condition) %>%
    addBMethod(blabel = "voom",
               bfunc = voom_pvals,
               countData = cntdat,
               group = coldat$condition,
               design = ~coldat$condition)
```

So far, none of the methods have been executed. The `BenchDesign` object simply serves as a container describing _how_ the methods should be executed. To apply each method to the data, we simply call `buildBench`.

```{r}
sb <- buildBench(bd, truthCols = "status")
```

We can inspect the results.

```{r}
sb
```

### Benchmark Evaluation

**Include SummarizedBenchmark stuff here (performance metrics, plots, subsetting/SummarizedExperiment features).**

## Advanced Use

Here, we describe..

### Multiple Outputs

The basic use case of `BenchDesign` described above has assumed that we are interested in a single numeric vector for each method, namely, a vector of adjusted p-values. These adjusted p-values are stored and returned as the sole `assay` in the returned `SummarizedBenchmark` object. However, in many cases, there can be multiple values of interest to be compared across methods. For example, the differential expression data set examined in the `SummarizedBenchmark` vignette includes two assays, one for q-values and a second for estimated log fold changes.  

The `BenchDesign` framework supports multiple assays with the `bpost` parameter of the `addBMethod` call. When no function or only a single function is specified to `bpost` for all methods, as in the examples above, the results from running each method are stored as a single `assay`. However, if `bpost` is passed a named list of functions, separate `assay`s will be created using each of the functions in the list. Since the `assay` names are taken from `bpost`, all entries in the list passed to `bpost` must be named. Furthermore, all methods must have the `bpost` parameter specified with lists with the same names and length. This is strictly enforced to avoid ambiguities when combining results across methods. 

```{r}
deseq2_run <- function(countData, colData, design, contrast) {
    dds <- DESeqDataSetFromMatrix(countData,
                                  colData = colData,
                                  design = design)
    dds <- DESeq(dds)
    results(dds, contrast = contrast)
}
deseq2_pv <- function(x) {
    x$pvalue
}
deseq2_lfc <- function(x) {
    x$log2FoldChange
}

edgeR_run <- function(countData, group, design) {
    y <- DGEList(countData, group = group)
    y <- calcNormFactors(y)
    des <- model.matrix(design)
    y <- estimateDisp(y, des)
    fit <- glmFit(y, des)
    glmLRT(fit, coef=2)
}
edgeR_pv <- function(x) {
    x$table$PValue
}
edgeR_lfc <- function(x) {
    x$table$logFC
}

voom_run <- function(countData, group, design) {
    y <- DGEList(countData, group = group)
    y <- calcNormFactors(y)
    des <- model.matrix(design)
    y <- voom(y, des)
    eBayes(lmFit(y, des))
}
voom_pv <- function(x) {
    x$p.value[, 2]
}
voom_lfc <- function(x) {
    x$coefficients[, 2]
}
```

```{r}
bd <- bd %>%
    addBMethod(blabel = "deseq2",
               bfunc = deseq2_run,
               bpost = list(pv = deseq2_pv, lfc = deseq2_lfc),
               countData = cntdat,
               colData = coldat, 
               design = ~condition,
               contrast = c("condition", "2", "1")) %>%
    addBMethod(blabel = "edgeR",
               bfunc = edgeR_run,
               bpost = list(pv = edgeR_pv, lfc = edgeR_lfc),
               countData = cntdat,
               group = coldat$condition,
               design = ~coldat$condition) %>%
    addBMethod(blabel = "voom",
               bfunc = voom_run,
               bpost = list(pv = voom_pv, lfc = voom_lfc),
               countData = cntdat,
               group = coldat$condition,
               design = ~coldat$condition)
```

When the `BenchDesign` is evaluated using `buildBench`, the resulting `SummarizedBenchmark` will be generated with two assays: `"pv"` and `"lfc"`. 

```{r}
sb <- buildBench(b = bd, truthCols = c(pv = "status", lfc = "lfc"))
sb
```

We can verify that the two assays contain the expected values. 

```{r}
assay(sb, "pv") %>% head 
assay(sb, "lfc") %>% head
```

### Parallelization with BiocParallel

The simple example considered in this vignette was constructed to be computational manageable with only one core. However, when working with larger data sets, running each method in serial on a single machine is often undesirable. Since constructing a `BenchDesign` object requires no computation, the bottleneck only appears at the `buildBench` step of the benchmarking process. Parallelization of this step is enabled using the `BiocParallel` package.  

By default, parallel evaluation is disabled, but can easily be enabled by setting `parallel = TRUE` and optionally specifying the `BPPARAM` parameter. If `BPPARAM` is not specified, the default back-end will be used. The default `BiocParallelParam` can be checked with `bpparam()`.

```{r}
bpparam()
sbp <- buildBench(bd, parallel = TRUE)
sbp
```

The results, as expected, are the same as when `buildBench` was called without parallelization.

```{r}
all(assay(sbp) == assay(sb), na.rm = TRUE)
```

Details on how to specify the parallelization back-end can be found in the _Introduction to BiocParallel_ vignette for the [`BiocParallel`](http://bioconductor.org/packages/release/bioc/html/BiocParallel.html) package. Parallelization of `buildBench` is carried out across the set of methods specified with `addBMethod`. There is no benefit to specifying more cores than the number of methods.

### Manually Specifying Method Metadata

**trivial example of including notes?**

Metadata for methods are stored in the `colData` of `SummarizedBenchmark` objects. As metioned above, several default metadata columns are populated in the `colData` of the `SummarizedBenchmark` object generated by a call to `buildBench`. Sometimes it may be useful to include additional metadata columns beyond the defaults. While this can be accomplished by manually modifying the `colData` of the `SummarizedBenchmark` object post hoc, method metadata can also be specified at the `addBMethod` step using the `bmeta` optional parameter. The `bmeta` parameter accepts a named list of metadata information. Each list entry will be added to the `colData` as columns. To avoid collisions between metadata columns specified with `bmeta` and the default set of columns, metadata specified using `bmeta` will be added to `colData` with `meta_` prefixed to the column name.  

As an example, we construct a `BenchDesign` object again using the `tdat` data set included with this package. The `BenchDesign` is created with two methods, `"bonf"` and `"BH"`. Each method is specified with the optional `bmeta` parameter.

```{r}
bd <- BenchDesign(tdat) %>%
    addBMethod(blabel = "bonf",
               bfunc = p.adjust,
               bmeta = list(reason = "recommended by friend X"),
               p = pval, method = "bonferroni") %>%
    addBMethod(blabel = "BH",
               bfunc = p.adjust,
               bmeta = list(reason = "recommended by friend Y"), 
               p = pval, method = "BH")
```

In this example, both methods have the additional `"ex1"` value specified, but only the `"BH"` method has the second `"random"` metadata column. While all methods in this `BenchDesign` have the `bmeta` option specified, this is not necessary. It is completely acceptable to specify the `bmeta` parameter for only a subset of methods.  

We can now verify that the manually defined metadata columns (`meta_ex1` and `meta_random`) are available in the `colData` of the generated `SummarizedBenchmark`.

```{r}
sb <- buildBench(b)
colData(sb)
```

### Manually Modifying Version Metadata

Arguably, two of the most important pieces of metadata stored in the `colData` of the `SummarizedBenchmark` returned by `buildBench` are the relevant package name and version (`pkg_name`, `pkg_vers`) for each method. Determining the package name and version requires the primary "workhorse" function of the method be directly specified as `bfunc` in the `addBMethod` call. In some cases, this may not be possible, e.g. if the "workhorse" function is not part of a package. However, there still might exists an important sub-function for which we would like to track the package name and version. The `bmeta` parameter can help.  

The `bmeta` parameter will handle the following named list entries separately: `pkg_name`, `pkg_vers`, `pkg_func`. First, if values are specified for `pkg_name` and `pkg_vers` in `bmeta`, these will overwrite the values determined from `bfunc`. To trace the source of `pkg_name` and `pkg_vers` information, the `vers_src` column of the `colData` will be set to `bmeta_manual` (the default value is `bfunc`). Alternatively, a function can be passed to `bmeta` as `pkg_func`. This function will be used to determine both `pkg_name` and `pkg_vers`, and will take precendence over manually specified `pkg_name` and `pkg_vers` values. If `pkg_func` is specified, it will be included in the `colData` as a new column with the same name, and the `vers_src` column will be set to `bmeta_func`. **Note: the function should be wrapped in `rlang::quo` to be properly parsed.  

The following example is completely unreasonable, but illustrates the behavior when using either `pkg_func` or `pkg_name` and `pkg_vers` with the `bmeta` optional parameter.

```{r}
b <- BenchDesign(tdat) %>%
    addBMethod(blabel = "bonf",
               bfunc = p.adjust,
               bmeta = list("pkg_func" = rlang::quo(stats::qnorm),
                            "random" = 100),
               p = pval, method = "bonferroni") %>%
    addBMethod(blabel = "BH",
               bfunc = p.adjust,
               bmeta = list("pkg_name" = "ggplot2", "pkg_vers" = "0.0.0.0.0",
                            "random" = 200), 
               p = pval, method = "BH")

sb <- buildBench(b)
colData(sb)
```

# References

- [Soneson C, Love MI and Robinson MD. 2016. Differential analyses for RNA-seq: transcript-level estimates improve gene-level inferences. F1000Research, 4:1521. (doi: 10.12688/f1000research.7563.2)](https://f1000research.com/articles/4-1521/v2)

---
title: "Building a Benchmark"
author: "Patrick Kimes"
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SummarizedBenchmark')`"
abstract: >
  benchmakr package version: `r packageVersion("SummarizedBenchmark")`
output:
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Building a benchmark with the BenchDesign framework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

When trying to decide how to perform a data analysis in R, it can be difficult to know which tool or method is the "best" for the problem. Benchmarking the performance of tools on real and simulated data sets is a common way of learning about the relative strengths and weaknesses of each approach. However, as the number of tools and parameters being compared increases, keeping track of output and how it was generated can quickly become messy. The **BenchDesign** class and associated functions implemented in `SummarizedBenchmark` provide one solution for this problem. 

```{r, echo = FALSE, message = FALSE}
library("SummarizedBenchmark")
library("magrittr")
```

# Simple Case Study

To illustrate the use of the `BenchDesign` class, we use the `tdat` data set included with this package. 

```{r}
data(tdat)
```

The data set is a data.frame containing the results of 50 two-sample t-tests. The tests were performed using independently simulated sets of 20 observations drawn from a single standard Normal distribution (when `H = 0`) or two mean-shifted Normal distributions (when `H = 1`).

```{r}
head(tdat)
```

Several approaches have been proposed and implemented to compute *adjusted p-values* with the goal of controling the total number of false discoveries across the collection of tests. In this example, we compare four such methods:

1. Bonferroni correction (`p.adjust` w/ `method = "bonferroni"`),
2. Benjamini-Hochberg (`p.adjust` w/ `method = "BH"`),
3. Storey's FDR q-value (`qvalue::qvalue`), and
4. the recently proposed Independent Hypothesis Weighting approach (`IHW::ihw`).


## Basic Approach

First, consider how benchmarking the four methods might look without the `BenchDesign` framework.  

To compare methods, each is applied to `tdat` and the results are stored in separate variables. 

```{r}
adjp_bonf <- p.adjust(p = tdat$pval, method = "bonferroni")

adjp_bh <- p.adjust(p = tdat$pval, method = "BH")

qv <- qvalue::qvalue(p = tdat$pval)
adjp_qv <- qv$qvalues

ihw_res <- IHW::ihw(pvalues = tdat$pval,
                     covariates = IHW::groups_by_filter(tdat$ind_covariate, 5),
                     alpha =  0.1)
adjp_ihw <- IHW::adj_pvalues(ihw_res) 
```

Since the output of each method is a vector of length 50 (the number of hypothesis tests), to keep things clean, they can be combined into a single data.frame.

```{r}
adjp <- cbind.data.frame(adjp_bonf, adjp_bh, adjp_qv, adjp_ihw)
head(adjp)
```

The data.frame of adjusted p-values can be used to evaluate each method. This can be done either by directly parsing the table, or by using a framework like `SummarizedBenchmark` or `iCOBRA`. Additionally, the data.frame can be saved as a `RDS` or `Rdata` object for future reference, reducing the need for recomputing on the original data.  

While this approach can work for smaller comparisons, it can quickly become overwhelming and unweildy as the number of methods and parameters included in the benchmark increases. Furthermore, once each method is applied and the final data.frame (`adj`) is constructed, there is no way to determine *how* each adjusted p-value was calculated. While an informative name can be used to "label" each method, this does not capture the full complexity, e.g. parameters and context, where the function was evaluated. One solution might involve manually recording function calls and parameters in a separate data.frame with the hope of maintaining synchrony with the output data.frame. However, this is notoriously prone to errors, e.g. during fast "copy and paste" operations or additions and delations of parameter combinations. An alternative (and hopefully better) solution, is to use the `BenchDesign` framework of the `SummarizedBenchmark` package.


## BenchDesign Approach 

In the `BenchDesign` framework implemented in the `SummarizedBenchmark` package, a `BenchDesign` is first constructed with the data as the sole input. (Alternatively, the `BenchDesign` can be constructed without any data input.)

```{r}
b <- BenchDesign(tdat)
```

Then, each method of interest is added to the `BenchDesign` using `addBMethod()`. 

```{r}
b <- BenchDesign(tdat)
b <- addBMethod(b, blabel = "bonf", bfunc = p.adjust,
                p = pval, method = "bonferroni")
```

At a minimum, `addBMethod()` requires specifying three parameters:

1. `b`: a `BenchDesign` object to modify,
2. `blabel`: a character name for the method, and
3. `bfunc`: the function to be called.

After the minimum parameters are specified, any parameters needed by the method specified in `bfunc` should be passed as named variables, e.g. `p = pval, method = "bonferroni"`. Notice that `pval` **does not** need to be called as `tdat$pval`.  

The process of adding methods can be written more concisely using the pipe operators from the `magrittr` package.

```{r}
b %<>% addBMethod(blabel = "BH", bfunc = p.adjust,
                  p = pval, method = "BH")
```

For some methods, such as the q-value and IHW approaches above, it may be necessary to call a "post-processing" function on the primary method call. This should be specified using the optional `bpost` parameter. 

```{r}
b %<>% addBMethod("qv", qvalue::qvalue,
                  p = pval,
                  bpost = function(x) { x$qvalues })

b %<>% addBMethod("ihw", IHW::ihw,
                  pvalues = pval,
                  covariates = IHW::groups_by_filter(ind_covariate, 5),
                  alpha = 0.1,
                  bpost = IHW::adj_pvalues)
```

Now, the `BenchDesign` object contains four methods. This can be verified using the `showBMethods()` function.

```{r}
showBMethods(b)
```

While the bench now includes all the information necessary for performing the benchmarking study, the actual adjusted p-values have not yet been calculated. To do this, we simply need to call `buildBench()`. This will return a `SummarizedBenchmark` object with a single "assay" containing the table of adjusted p-values.

```{r}
sb <- buildBench(b)
sb
```

```{r}
head(assay(sb))
```

The `colData` of the same object contains information on the methods (columns) of each assay, with each row of the `colData` corresponding to a column in the `assay`. The `colData` includes columns for the functions and parameters specified for each method with `addBMethod` (`bfunc, bpost, blabel, param.*`). Additionally, several other columns are included from the `buildBench` process. These include a column specifying whether `bfunc` was an anonymous function (`bfunc_anon`), and columns for the package name and version of `bfunc` if available (`pkg_name`, `pkg_vers`).

```{r}
colData(sb)
```

The resulting object can now be saved to disk with the method information tied directly to the resulting output. Additionally, comparison of the adjusted p-values can now be carried out using downstream methods using the `SummarizedBenchmark` framework.


# Advanced Usage

## Parallelization with BiocParallel

The simple example considered in this vignette was constructed to be computational manageable with only one core. However, when working with larger data sets, running each method in serial on a single machine is often undesirable. Since constructing a `BenchDesign` object requires no computation, the bottleneck only appears at the `buildBench` step of the benchmarking process. Parallelization of this step is enabled using the `BiocParallel` package.  

By default, parallel evaluation is disabled, but can easily be enabled by setting `parallel = TRUE` and optionally specifying the `BPPARAM` parameter. If `BPPARAM` is not specified, the default back-end will be used. The default `BiocParallelParam` can be checked with `bpparam()`.

```{r}
bpparam()
sbp <- buildBench(b, parallel = TRUE)
sbp
```

The results, as expected, are the same as when `buildBench` was called without parallelization.

```{r}
all(assay(sbp) == assay(sb))
```

Details on how to specify the parallelization back-end can be found in the _Introduction to BiocParallel_ vignette for the [`BiocParallel`](http://bioconductor.org/packages/release/bioc/html/BiocParallel.html) package. Parallelization of `buildBench` is carried out across the set of methods specified with `addBMethod`. There is no benefit to specifying more cores than the number of methods.

## Manually Specifying Method Metadata

Metadata for methods are stored in the `colData` of `SummarizedBenchmark` objects. As metioned above, several default metadata columns are populated in the `colData` of the `SummarizedBenchmark` object generated by a call to `buildBench`. Sometimes it may be useful to include additional metadata columns beyond the defaults. While this can be accomplished by manually modifying the `colData` of the `SummarizedBenchmark` object post hoc, method metadata can also be specified at the `addBMethod` step using the `bmeta` optional parameter. The `bmeta` parameter accepts a named list of metadata information. Each list entry will be added to the `colData` as columns. To avoid collisions between metadata columns specified with `bmeta` and the default set of columns, metadata specified using `bmeta` will be added to `colData` with `meta_` prefixed to the column name.  

As an example, we construct a `BenchDesign` object again using the `tdat` data set included with this package. The `BenchDesign` is created with two methods, `"bonf"` and `"BH"`. Each method is specified with the optional `bmeta` parameter.

```{r}
b <- BenchDesign(tdat) %>%
    addBMethod(blabel = "bonf",
               bfunc = p.adjust,
               bmeta = list("ex1" = "manually"),
               p = pval, method = "bonferroni") %>%
    addBMethod(blabel = "BH",
               bfunc = p.adjust,
               bmeta = list("random" = 100,
                            "ex1" = "specified"), 
               p = pval, method = "BH")
```

In this example, both methods have the additional `"ex1"` value specified, but only the `"BH"` method has the second `"random"` metadata column. While all methods in this `BenchDesign` have the `bmeta` option specified, this is not necessary. It is completely acceptable to specify the `bmeta` parameter for only a subset of methods.  

We can now verify that the manually defined metadata columns (`meta_ex1` and `meta_random`) are available in the `colData` of the generated `SummarizedBenchmark`.

```{r}
sb <- buildBench(b)
colData(sb)
```

## Manually Modifying Version Metadata

Arguably, two of the most important pieces of metadata stored in the `colData` of the `SummarizedBenchmark` returned by `buildBench` are the relevant package name and version (`pkg_name`, `pkg_vers`) for each method. Determining the package name and version requires the primary "workhorse" function of the method be directly specified as `bfunc` in the `addBMethod` call. In some cases, this may not be possible, e.g. if the "workhorse" function is not part of a package. However, there still might exists an important sub-function for which we would like to track the package name and version. The `bmeta` parameter can help.  

The `bmeta` parameter will handle the following named list entries separately: `pkg_name`, `pkg_vers`, `pkg_func`. First, if values are specified for `pkg_name` and `pkg_vers` in `bmeta`, these will overwrite the values determined from `bfunc`. To trace the source of `pkg_name` and `pkg_vers` information, the `vers_src` column of the `colData` will be set to `bmeta_manual` (the default value is `bfunc`). Alternatively, a function can be passed to `bmeta` as `pkg_func`. This function will be used to determine both `pkg_name` and `pkg_vers`, and will take precendence over manually specified `pkg_name` and `pkg_vers` values. If `pkg_func` is specified, it will be included in the `colData` as a new column with the same name, and the `vers_src` column will be set to `bmeta_func`. **Note: the function should be wrapped in `rlang::quo` to be properly parsed.  

The following example is completely unreasonable, but illustrates the behavior when using either `pkg_func` or `pkg_name` and `pkg_vers` with the `bmeta` optional parameter.

```{r}
b <- BenchDesign(tdat) %>%
    addBMethod(blabel = "bonf",
               bfunc = p.adjust,
               bmeta = list("pkg_func" = rlang::quo(stats::qnorm),
                            "random" = 100),
               p = pval, method = "bonferroni") %>%
    addBMethod(blabel = "BH",
               bfunc = p.adjust,
               bmeta = list("pkg_name" = "ggplot2", "pkg_vers" = "0.0.0.0.0",
                            "random" = 200), 
               p = pval, method = "BH")

sb <- buildBench(b)
colData(sb)
```

## Generating Multiple Metrics/Assays with BenchDesign 

The basic use case of `BenchDesign` described above has assumed that we are interested in a single numeric vector for each method, namely, a vector of adjusted p-values. These adjusted p-values are stored and returned as the sole `assay` in the returned `SummarizedBenchmark` object. However, in many cases, there can be multiple values of interest to be compared across methods. For example, the differential expression data set examined in the `SummarizedBenchmark` vignette includes two assays, one for q-values and a second for estimated log fold changes.  

The `BenchDesign` framework supports multiple assays with the `bpost` parameter of the `addBMethod` call. When no function or only a single function is specified to `bpost` for all methods, as in the examples above, the results from running each method are stored as a single `assay`. However, if `bpost` is passed a named list of functions, separate `assay`s will be created using each of the functions in the list. Since the `assay` names are taken from `bpost`, all entries in the list passed to `bpost` must be named. Furthermore, all methods must have the `bpost` parameter specified with lists with the same names and length. This is strictly enforced to avoid ambiguities when combining results across methods. 

A trivial example of using the multi-assay support through `bpost` is given for the `tdat` data set. 

```{r}
b <- BenchDesign(tdat) %>%
    addBMethod(blabel = "bonf",
               bfunc = p.adjust,
               bpost = list(regular = function(x) { x },
                            negative = function(x) { -x }),
               p = pval, method = "bonferroni") %>%
    addBMethod(blabel = "BH",
               bfunc = p.adjust,
               bpost = list(regular = function(x) { x },
                            negative = function(x) { -x }),
               p = pval, method = "BH")
```

When the `BenchDesign` is evaluated using `buildBench`, the resulting `SummarizedBenchmark` will be generated with two assays: `"regular"` and `"negative"`. 

```{r}
sb <- buildBench(b)
sb
```

We can verify that the two assays contain the expected values. In this case, the entries of the `"negative"` assay should be the negated values of the `"regular"` assay.

```{r}
assay(sb, "regular") %>% head 
assay(sb, "negative") %>% head
```
